## 神经网络

[神经网络浅讲：从神经元到深度学习 - 计算机的潜意识 - 博客园](https://www.cnblogs.com/subconscious/p/5058741.html)

1. 设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；

2. 神经网络结构图中的拓扑与箭头代表着**预测**过程时数据的流向，跟**训练**时的数据流有一定的区别；

3. 结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的**权重**（其值称为**权值**），这是需要训练得到的。 

   

理论证明，两层神经网络可以无限逼近任意连续函数。也就是说，面对复杂的**非线性分类任务**，两层（带一个隐藏层）神经网络可以分类的很好。

更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。

在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**过拟合现象**。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250116105729233.png" alt="image-20250116105729233" style="zoom:50%;" />



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250116105908416.png" alt="image-20250116105908416" style="zoom:50%;" />

神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250116105959093.png" alt="image-20250116105959093" style="zoom:50%;" />



## 卷积神经网络（CNN）

一个卷积神经网络主要由以下5层组成：

### 数据输入层/ Input layer

​	处理主要是对原始图像数据进行预处理，其中包括

​	**去均值**：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。

​	**归一化**：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰

​	**PCA/白化**：用PCA降维；白化是对数据各个特征轴上的幅度归一化



### 卷积计算层/ CONV layer

​	在这个卷积层，有两个关键操作：

​	**局部关联**：每个神经元看做一个滤波器(filter)

​	**窗口(receptive field)滑动**： filter对局部数据计算



### ReLU激励层 / ReLU layer

### 池化层/Pooling layer

​	池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合	//  特征不变性 特征降维 防止过拟合

提取到最有代表性的特征

最大池化

​	顾名思义，最大池化就是每次取正方形中所有值的最大值，这个最大值也就相当于当前位置最具有代表性的特征

平均池化

​	平均池化就是取此正方形区域中所有值的平均值，考虑到每个位置的值对于此处特征的影响，平均池化计算也比较简单

### 全连接层/ FC layer

​	两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部

## Transformer

## Machine Learning == Looking for Function



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121143726780.png" alt="image-20250121143726780" style="zoom:25%;" />



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121144126427.png" alt="image-20250121144126427" style="zoom:25%;" />



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121144220542.png" alt="image-20250121144220542" style="zoom:25%;" />

机器学习就是定一个有位置参数的方程，通过机器的学习找出对应的方程

loss是自己定义的。求导的值决定移动的位置，找到最小的loss

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121145215677.png" alt="image-20250121145215677" style="zoom:25%;" />



微分pytorch可以自动求！

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121145318798.png" alt="image-20250121145318798" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121145448405.png" alt="image-20250121145448405" style="zoom:25%;" />

这个步骤叫训练

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121145539449.png" alt="image-20250121145539449" style="zoom:25%;" />

上面是简单的线性模型，太简单了

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121150315832.png" alt="image-20250121150315832" style="zoom:25%;" />



经典

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121150831563.png" alt="image-20250121150831563" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121150948069.png" alt="image-20250121150948069" style="zoom:25%;" />

实际要红色，红色可以用多个蓝色相加得出。对应分段拟合（弯曲的函数可以用直线拟合）！

线性model有限制，因此使用Sigmoid解除限制！

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121151012754.png" alt="image-20250121151012754" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121151059570.png" alt="image-20250121151059570" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121151931760.png" alt="image-20250121151931760" style="zoom:25%;" />

sigmoid越多，拟合出来的函数越复杂！

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121151832071.png" alt="image-20250121151832071" style="zoom:25%;" />

修改好后的定义MLFramework

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121152156801.png" alt="image-20250121152156801" style="zoom:25%;" />



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121153205986.png" alt="image-20250121153205986" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121153147623.png" alt="image-20250121153147623" style="zoom:25%;" />



<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121153735507.png" alt="image-20250121153735507" style="zoom:25%;" />

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121153953560.png" alt="image-20250121153953560" style="zoom:25%;" />

## DeepLearning

<img src="C:\Users\HBY\AppData\Roaming\Typora\typora-user-images\image-20250121162802771.png" alt="image-20250121162802771" style="zoom:25%;" />





  







 
